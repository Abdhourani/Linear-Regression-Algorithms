# Regression Optimization Techniques

This project demonstrates various gradient descent techniques for linear regression, including Gradient Descent (GD), Stochastic Gradient Descent (SGD), Momentum, RMSprop, and ADAM optimization. The algorithms are tested on synthetic data, visualizing the results to help understand their performance in fitting a linear model.

## Table of Contents
1. [Project Overview](#project-overview)
2. [Installation](#installation)
3. [Usage](#usage)
4. [Optimization Techniques](#optimization-techniques)
5. [Example Output](#example-output)
6. [References](#references)

## Project Overview

The code generates synthetic linear data, optionally adds noise and outliers, and applies several optimization algorithms to fit a line to this data. Each technique aims to minimize the Mean Squared Error (MSE) between predicted and actual values. The following optimizers are implemented:

- **Gradient Descent (GD)**
- **Stochastic Gradient Descent (SGD)**
- **Momentum**
- **RMSprop**
- **ADAM**

## Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/Abdhourani/Linear-Regression-Algorithms.git
